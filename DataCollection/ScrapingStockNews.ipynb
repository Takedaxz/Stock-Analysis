{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TqhEjAwTgWKr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import cloudscraper\n",
        "from htmldate import find_date\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 10 links per page, expecting 20 total\n",
            "All pages OK after 0 retries\n",
            "Total links fetched (including duplicates): 20 (expected 20)\n",
            "Final: got 20 unique URLs (expected 20)\n"
          ]
        }
      ],
      "source": [
        "###CONFIGURATION\n",
        "COMPANY     = \"us-spx-500\"   \n",
        "MAX_PAGE    = 2\n",
        "MAX_WORKERS = 50              \n",
        "MAX_RETRIES = 5              \n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": (\n",
        "        \"text/html,application/xhtml+xml,application/xml;\"\n",
        "        \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
        "    ),\n",
        "    \"Referer\": \"https://www.investing.com/\",\n",
        "}\n",
        "\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows'}\n",
        ")\n",
        "\n",
        "def fetch_page(page: int):\n",
        "    global ticker\n",
        "    #Equity\n",
        "    #url = f\"https://www.investing.com/equities/{COMPANY}-news/{page}\"\n",
        "    #Index\n",
        "    url = f\"https://www.investing.com/indices/{COMPANY}-news/{page}\"\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, \"lxml\")\n",
        "            h1_tag = soup.find('h1', class_='mb-2.5') # Use a specific class or combination of classes for robustness\n",
        "            full_text = h1_tag.text.strip()\n",
        "            match = re.search(r'\\(([^)]+)\\)', full_text)\n",
        "            ticker = match.group(1)\n",
        "                    \n",
        "            anchors = soup.select(\n",
        "                'ul[data-test=\"news-list\"] '\n",
        "                'li article a[data-test=\"article-title-link\"]'\n",
        "            )\n",
        "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_RETRIES:\n",
        "                backoff = 2 ** (attempt - 1) + random.random()\n",
        "                time.sleep(backoff)\n",
        "            else:\n",
        "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
        "    return []\n",
        "\n",
        "def robust_scrape():\n",
        "    first = fetch_page(1)\n",
        "    PER_PAGE = len(first)\n",
        "    if PER_PAGE == 0:\n",
        "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies and try again.\")\n",
        "    print(f\"Detected {PER_PAGE} links per page, expecting {PER_PAGE * MAX_PAGE} total\")\n",
        "\n",
        "    results = {1: first}\n",
        "    pages = list(range(2, MAX_PAGE + 1))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
        "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
        "        for fut in as_completed(futures):\n",
        "            p = futures[fut]\n",
        "            results[p] = fut.result()\n",
        "\n",
        "        for round in range(1, MAX_RETRIES + 1):\n",
        "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
        "            if not bad:\n",
        "                print(f\"All pages OK after {round-1} retries\")\n",
        "                break\n",
        "            print(f\"Retry round {round} for pages: {bad}\")\n",
        "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
        "            for fut in as_completed(futures):\n",
        "                p = futures[fut]\n",
        "                results[p] = fut.result()\n",
        "        else:\n",
        "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
        "\n",
        "    total_fetched = sum(len(links) for links in results.values())\n",
        "    expected = PER_PAGE * MAX_PAGE\n",
        "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
        "\n",
        "    all_links = set(link for links in results.values() for link in links)\n",
        "    print(f\"Final: got {len(all_links)} unique URLs (expected {expected})\")\n",
        "    return list(all_links)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    links = robust_scrape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://www.investing.com/news/stock-market-news/us-stock-futures-muted-as-lower-yields-bring-little-relief-debt-concerns-persist-4061108'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Fetch][12/20][ok]\n",
            "[Fetch][11/20][ok]\n",
            "[Fetch][1/20][ok]\n",
            "[Fetch][8/20][ok]\n",
            "[Fetch][15/20][ok]\n",
            "[Fetch][6/20][ok]\n",
            "[Fetch][14/20][ok]\n",
            "[Fetch][19/20][ok]\n",
            "[Fetch][13/20][ok]\n",
            "[Fetch][7/20][ok]\n",
            "[Fetch][16/20][ok]\n",
            "[Fetch][4/20][ok]\n",
            "[Fetch][18/20][ok]\n",
            "[Fetch][3/20][ok]\n",
            "[Fetch][9/20][ok]\n",
            "[Fetch][2/20][ok]\n",
            "[Fetch][20/20][ok]\n",
            "[Fetch][17/20][ok]\n",
            "[Fetch][5/20][ok]\n",
            "[Fetch][10/20][ok]\n",
            "[Process][1/20] https://www.investing.com/news/pro/morgan-stanley-sees-this-sector-leading-from-us-infrastructure-push-432SI-4062403\n",
            "[Process][2/20] https://www.investing.com/news/stock-market-news/us-stock-futures-muted-as-lower-yields-bring-little-relief-debt-concerns-persist-4061108\n",
            "[Process][3/20] https://www.investing.com/news/pro/unitedhealth-stock-what-is-the-path-from-here-432SI-4063134\n",
            "[Process][4/20] https://www.investing.com/news/stock-market-news/tsx-opens-lower-amid-more-trump-trade-turmoil-4062094\n",
            "[Process][5/20] https://www.investing.com/news/economy-news/asian-shares-make-cautious-gains-as-beatendown-treasuries-find-support-4061147\n",
            "[Process][6/20] https://www.investing.com/news/stock-market-news/jbs-minority-shareholders-approve-dual-usbrazil-listing-4062054\n",
            "[Process][7/20] https://www.investing.com/news/stock-market-news/nvidia-earnings-in-focus-as-rising-us-yields-debt-rattle-markets-4061611\n",
            "[Process][8/20] https://www.investing.com/news/economy-news/is-a-us-government-debt-crisis-imminent-4059905\n",
            "[Process][9/20] https://www.investing.com/news/economy-news/trading-day-trump-shatters-tariff-calm-with-new-salvo-4062548\n",
            "[Process][10/20] https://www.investing.com/news/stock-market-news/is-this-a-turning-point-for-markets-4063585\n",
            "[Process][11/20] https://www.investing.com/news/stock-market-news/us-stock-futures-jump-as-trump-agrees-to-postpone-50-eu-tariffs-4063319\n",
            "[Process][12/20] https://www.investing.com/news/economy-news/trump-says-he-is-not-happy-with-putin-over-ukraine-strikes-mulls-new-sanctions-4063306\n",
            "[Process][13/20] https://www.investing.com/news/stock-market-news/nvidia-earnings-in-focus-as-rising-us-yields-debt-rattle-markets-4063190\n",
            "[Process][14/20] https://www.investing.com/news/stock-market-news/asia-stocks-mixed-amid-trump-tariff-uncertainty-japan-extends-gains-4063344\n",
            "[Process][15/20] https://www.investing.com/news/economy-news/take-five-chips-and-trouble-in-bond-land-4063440\n",
            "[Process][16/20] https://www.investing.com/news/economy-news/trump-aims-to-use-tariffs-to-boost-manufacturing-jobs-but-is-it-possible-4057101\n",
            "[Process][17/20] https://www.investing.com/news/press-releases/jpmorgan-announces-cash-distributions-for-the-jpmorgan-etfs-93CH-4063625\n",
            "[Process][18/20] https://www.investing.com/news/stock-market-news/us-stock-futures-flat-on-debt-worries-as-markets-mull-trumps-tax-bill-4061588\n",
            "[Process][19/20] https://www.investing.com/news/pro/what-can-get-the-us-largecap-pharma-sector-to-work-432SI-4063049\n",
            "[Process][20/20] https://www.investing.com/news/stock-market-news/us-stocks-lower-at-close-of-trade-dow-jones-industrial-average-down-061-4062523\n"
          ]
        }
      ],
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
        "PROCESS_WORKERS = os.cpu_count() or 4\n",
        "MAX_FETCH_RETRIES = 3                      \n",
        "RETRY_DELAY = 1                             \n",
        "TICKER = ticker\n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "def is_placeholder(html: str) -> bool:\n",
        "    lower = html.lower() if html else \"\"\n",
        "    return (\n",
        "        'temporarily down for maintenance' in lower\n",
        "        or 'just a moment' in lower\n",
        "        or \"we're temporarily down\" in lower\n",
        "    )\n",
        "\n",
        "def safe_find_datetime(url, html_content=None):\n",
        "    try:\n",
        "        # Strategy 1: Use htmldate library to extract date from URL\n",
        "        dt = find_date(url)\n",
        "        if dt:\n",
        "            return dt, \"00:00\"  # Return with default time if date found\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    if html_content:\n",
        "        # Strategy 2: Look for American format with AM/PM\n",
        "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            try:\n",
        "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
        "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Strategy 3: Look for numeric dates with 24-hour time format\n",
        "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            # Try both European and American date formats\n",
        "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
        "                try:\n",
        "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
        "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
        "\n",
        "def fetch_html(url, idx, total):\n",
        "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
        "        try:\n",
        "            resp = scraper.get(url, timeout=30)\n",
        "            html = resp.text\n",
        "            if is_placeholder(html):\n",
        "                raise RuntimeError('Placeholder')\n",
        "                \n",
        "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
        "            return url, html\n",
        "            \n",
        "        except Exception:\n",
        "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
        "            if attempt < MAX_FETCH_RETRIES:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                \n",
        "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
        "    return url, None\n",
        "\n",
        "def process_article(arg):\n",
        "    url, html = arg\n",
        "    if not html:\n",
        "        return None\n",
        "        \n",
        "    art = Article(url)\n",
        "    art.set_html(html)\n",
        "    \n",
        "    try:\n",
        "        art.parse()\n",
        "    except:\n",
        "        return None\n",
        "        \n",
        "    text = art.text or \"\"\n",
        "    title = (art.title or \"\").strip() or \"No title\"\n",
        "    \n",
        "    date, tm = safe_find_datetime(url, html)\n",
        "    \n",
        "    # Return combined data using dictionary unpacking\n",
        "    return {'ticker': TICKER, 'publish_date': date, 'publish_time': tm,\n",
        "             'title': title, 'body_text': text, 'url': url}\n",
        "\n",
        "async def scrape_all(urls):\n",
        "    total = len(urls)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    \n",
        "    # Phase 1: Fetch HTML content from all URLs in parallel\n",
        "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
        "        # Create fetch tasks and run them through the thread pool\n",
        "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
        "                       for i, u in enumerate(urls)]\n",
        "        # Wait for all fetch tasks to complete\n",
        "        fetched = await asyncio.gather(*fetch_tasks)\n",
        "\n",
        "    # Phase 2: Process all fetched HTML content in parallel\n",
        "    records = []\n",
        "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
        "        # Submit processing tasks only for URLs with successful fetches\n",
        "        futures = {\n",
        "            proc_pool.submit(process_article, fr): fr[0]\n",
        "            for fr in fetched if fr[1]  # Skip URLs where HTML is None\n",
        "        }\n",
        "        \n",
        "        # Process results as they complete\n",
        "        for i, fut in enumerate(as_completed(futures), 1):\n",
        "            res = fut.result()\n",
        "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
        "            if res:\n",
        "                records.append(res)\n",
        "                \n",
        "    # Convert results to DataFrame\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# ——— Main entry point function ———\n",
        "def main(links):\n",
        "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
        "    return df\n",
        "\n",
        "# Execute the main function if this script is run directly\n",
        "if __name__ == '__main__':\n",
        "    df = main(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ticker</th>\n",
              "      <th>publish_date</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body_text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SPX</td>\n",
              "      <td>2025-05-26</td>\n",
              "      <td>07:12</td>\n",
              "      <td>JPMorgan Announces Cash Distributions for the ...</td>\n",
              "      <td>TORONTO, May 26, 2025 (GLOBE NEWSWIRE) -- J.P....</td>\n",
              "      <td>https://www.investing.com/news/press-releases/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPX</td>\n",
              "      <td>2025-05-26</td>\n",
              "      <td>06:25</td>\n",
              "      <td>Is this a turning point for markets? By Invest...</td>\n",
              "      <td>Investing.com -- Despite a turbulent start to ...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SPX</td>\n",
              "      <td>2025-05-26</td>\n",
              "      <td>02:31</td>\n",
              "      <td>Take Five: Chips and trouble in bond land By R...</td>\n",
              "      <td>(Reuters) -Chip behemoth Nvidia (NASDAQ: ) is ...</td>\n",
              "      <td>https://www.investing.com/news/economy-news/ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SPX</td>\n",
              "      <td>2025-05-25</td>\n",
              "      <td>22:40</td>\n",
              "      <td>Asia stocks mixed amid Trump tariff uncertaint...</td>\n",
              "      <td>Investing.com-- Most Asian markets were a mixe...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SPX</td>\n",
              "      <td>2025-05-25</td>\n",
              "      <td>20:41</td>\n",
              "      <td>US stock futures jump as Trump agrees to postp...</td>\n",
              "      <td>Investing.com-- U.S. stock index futures rose ...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ticker publish_date publish_time  \\\n",
              "0    SPX   2025-05-26        07:12   \n",
              "1    SPX   2025-05-26        06:25   \n",
              "2    SPX   2025-05-26        02:31   \n",
              "3    SPX   2025-05-25        22:40   \n",
              "4    SPX   2025-05-25        20:41   \n",
              "\n",
              "                                               title  \\\n",
              "0  JPMorgan Announces Cash Distributions for the ...   \n",
              "1  Is this a turning point for markets? By Invest...   \n",
              "2  Take Five: Chips and trouble in bond land By R...   \n",
              "3  Asia stocks mixed amid Trump tariff uncertaint...   \n",
              "4  US stock futures jump as Trump agrees to postp...   \n",
              "\n",
              "                                           body_text  \\\n",
              "0  TORONTO, May 26, 2025 (GLOBE NEWSWIRE) -- J.P....   \n",
              "1  Investing.com -- Despite a turbulent start to ...   \n",
              "2  (Reuters) -Chip behemoth Nvidia (NASDAQ: ) is ...   \n",
              "3  Investing.com-- Most Asian markets were a mixe...   \n",
              "4  Investing.com-- U.S. stock index futures rose ...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.investing.com/news/press-releases/...  \n",
              "1  https://www.investing.com/news/stock-market-ne...  \n",
              "2  https://www.investing.com/news/economy-news/ta...  \n",
              "3  https://www.investing.com/news/stock-market-ne...  \n",
              "4  https://www.investing.com/news/stock-market-ne...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of articles with empty body_text: 0\n"
          ]
        }
      ],
      "source": [
        "# Count empty body_text entries\n",
        "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
        "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y-%m-%d %H-%M-%S\").strip().replace(' ', '_')\n",
        "df.to_csv(f\"Data/News/{TICKER}_{date_time}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2021-06-19 Stock news data collection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
