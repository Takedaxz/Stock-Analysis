{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping from Investing.com\n",
        "- Scrape from trending news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TqhEjAwTgWKr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import cloudscraper\n",
        "from htmldate import find_date\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 20 links on page 1\n",
            "Only one page detected or no pagination found.\n"
          ]
        }
      ],
      "source": [
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# CONFIGURATION\n",
        "MAX_WORKERS = 10\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": (\n",
        "        \"text/html,application/xhtml+xml,application/xml;\"\n",
        "        \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
        "    ),\n",
        "    \"Referer\": \"https://www.investing.com/\",\n",
        "}\n",
        "\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows'}\n",
        ")\n",
        "\n",
        "def fetch_page(page: int):\n",
        "    # If page == 1, use default URL, else use pagination format\n",
        "    url = \"https://www.investing.com/news/most-popular-news\"\n",
        "    if page > 1:\n",
        "        url += f\"/{page}\"\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "            anchors = soup.select(\n",
        "                'ul[data-test=\"news-list\"] '\n",
        "                'li article a[data-test=\"article-title-link\"]'\n",
        "            )\n",
        "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_RETRIES:\n",
        "                backoff = 2 ** (attempt - 1) + random.random()\n",
        "                time.sleep(backoff)\n",
        "            else:\n",
        "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
        "    return []\n",
        "\n",
        "def robust_scrape(max_pages=10):\n",
        "    first = fetch_page(1)\n",
        "    PER_PAGE = len(first)\n",
        "    if PER_PAGE == 0:\n",
        "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies.\")\n",
        "\n",
        "    print(f\"Fetched {PER_PAGE} links on page 1\")\n",
        "\n",
        "    results = {1: first}\n",
        "\n",
        "    # Auto stop if only one page\n",
        "    if max_pages == 1 or PER_PAGE == 0:\n",
        "        print(\"Only one page detected or no pagination found.\")\n",
        "        return first\n",
        "\n",
        "    pages = list(range(2, max_pages + 1))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
        "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
        "        for fut in as_completed(futures):\n",
        "            p = futures[fut]\n",
        "            results[p] = fut.result()\n",
        "\n",
        "        for round in range(1, MAX_RETRIES + 1):\n",
        "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
        "            if not bad:\n",
        "                print(f\"All pages OK after {round - 1} retries\")\n",
        "                break\n",
        "            print(f\"Retry round {round} for pages: {bad}\")\n",
        "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
        "            for fut in as_completed(futures):\n",
        "                p = futures[fut]\n",
        "                results[p] = fut.result()\n",
        "        else:\n",
        "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
        "\n",
        "    total_fetched = sum(len(links) for links in results.values())\n",
        "    expected = PER_PAGE * max_pages\n",
        "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
        "\n",
        "    all_links = set(link for links in results.values() for link in links)\n",
        "    print(f\"Final: got {len(all_links)} unique URLs\")\n",
        "    return list(all_links)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    links = robust_scrape(max_pages=1)  # You can adjust this to the actual known number of pages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://www.investing.com/news/stock-market-news/25-tariff-not-enough-to-push-apple-to-reshore-iphone-production--morgan-stanley-4064984'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Fetch][11/20][retry 1]\n",
            "[Fetch][19/20][retry 1]\n",
            "[Fetch][14/20][retry 1]\n",
            "[Fetch][2/20][ok]\n",
            "[Fetch][3/20][ok]\n",
            "[Fetch][10/20][ok]\n",
            "[Fetch][1/20][ok]\n",
            "[Fetch][12/20][ok]\n",
            "[Fetch][15/20][ok]\n",
            "[Fetch][4/20][ok]\n",
            "[Fetch][16/20][ok]\n",
            "[Fetch][17/20][ok]\n",
            "[Fetch][9/20][ok]\n",
            "[Fetch][13/20][ok]\n",
            "[Fetch][5/20][ok]\n",
            "[Fetch][7/20][ok]\n",
            "[Fetch][20/20][ok]\n",
            "[Fetch][8/20][ok]\n",
            "[Fetch][18/20][ok]\n",
            "[Fetch][6/20][ok]\n",
            "[Fetch][11/20][ok]\n",
            "[Fetch][14/20][ok]\n",
            "[Fetch][19/20][ok]\n",
            "[Process][1/20] https://www.investing.com/news/economy-news/what-could-be-the-impact-of-a-50tariff-on-eu-economy-4064349\n",
            "[Process][2/20] https://www.investing.com/news/stock-market-news/next-300-points-likely-up-for-the-sp-500-says-jpmorgan-4064865\n",
            "[Process][3/20] https://www.investing.com/news/stock-market-news/new-surveys-show-declining-interest-in-evs-and-the-tesla-brand-around-the-world-4064783\n",
            "[Process][4/20] https://www.investing.com/news/stock-market-news/citi-sees-up-to-8-downside-for-stoxx-600-on-trump-tariff-risk-4064531\n",
            "[Process][5/20] https://www.investing.com/news/stock-market-news/25-tariff-not-enough-to-push-apple-to-reshore-iphone-production--morgan-stanley-4064984\n",
            "[Process][6/20] https://www.investing.com/news/cryptocurrency-news/bitcoin-price-today-rangebound-at-109k-post-record-high-2025-crypto-summit-eyed-4064028\n",
            "[Process][7/20] https://www.investing.com/news/economy-news/higher-us-futures-new-nvidia-chip-falling-tesla-sales--whats-moving-markets-4064122\n",
            "[Process][8/20] https://www.investing.com/news/economy-news/us-consumer-bounces-back-sharply-in-may-4065501\n",
            "[Process][9/20] https://www.investing.com/news/stock-market-news/lynx-risk-priced-in-nvidia-could-surprise-to-the-upside-4065322\n",
            "[Process][10/20] https://www.investing.com/news/commodities-news/gold-prices-sink-as-trump-eu-tariff-delay-spurs-some-risk-yields-retreat-4064017\n",
            "[Process][11/20] https://www.investing.com/news/stock-market-news/asia-stocks-muted-amid-trade-caution-japan-dips-on-boj-rate-hike-comments-4063930\n",
            "[Process][12/20] https://www.investing.com/news/stock-market-news/us-stock-futures-jump-as-trump-delays-50-tariffs-against-the-eu-4063878\n",
            "[Process][13/20] https://www.investing.com/news/stock-market-news/temu-owner-pdd-holdings-shares-tank-after-big-q1-results-miss-4064614\n",
            "[Process][14/20] https://www.investing.com/news/commodities-news/citi-raises-shortterm-gold-price-target-to-3500-amid-tariff-concerns-4063419\n",
            "[Process][15/20] https://www.investing.com/news/stock-market-news/soundhound-shares-jump-as-piper-sandler-starts-coverage-at-overweight-4065185\n",
            "[Process][16/20] https://www.investing.com/news/stock-market-news/eli-lilly-expands-pain-pipeline-with-acquisition-of-siteone-therapeutics-4065495\n",
            "[Process][17/20] https://www.investing.com/news/stock-market-news/byd-shares-slide-further-from-record-highs-amid-price-cuts-competition-concerns-4063996\n",
            "[Process][18/20] https://www.investing.com/news/cryptocurrency-news/trump-media--technology-group-announces-25b-btc-treasury-deal-432SI-4065162\n",
            "[Process][19/20] https://www.investing.com/news/stock-market-news/tesla-europe-sales-nearly-halve-in-april-as-competition-musk-boycott-persist-4063962\n",
            "[Process][20/20] https://www.investing.com/news/cryptocurrency-news/jpmorgan-shifts-to-bullish-stance-on-crypto-amid-sector-tailwinds-4064834\n"
          ]
        }
      ],
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
        "PROCESS_WORKERS = os.cpu_count() or 4\n",
        "MAX_FETCH_RETRIES = 3                      \n",
        "RETRY_DELAY = 1                             \n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "def is_placeholder(html: str) -> bool:\n",
        "    lower = html.lower() if html else \"\"\n",
        "    return (\n",
        "        'temporarily down for maintenance' in lower\n",
        "        or 'just a moment' in lower\n",
        "        or \"we're temporarily down\" in lower\n",
        "    )\n",
        "\n",
        "def safe_find_datetime(url, html_content=None):\n",
        "    try:\n",
        "        # Strategy 1: Use htmldate library to extract date from URL\n",
        "        dt = find_date(url)\n",
        "        if dt:\n",
        "            return dt, \"00:00\"  # Return with default time if date found\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    if html_content:\n",
        "        # Strategy 2: Look for American format with AM/PM\n",
        "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            try:\n",
        "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
        "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Strategy 3: Look for numeric dates with 24-hour time format\n",
        "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            # Try both European and American date formats\n",
        "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
        "                try:\n",
        "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
        "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
        "\n",
        "def fetch_html(url, idx, total):\n",
        "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
        "        try:\n",
        "            resp = scraper.get(url, timeout=30)\n",
        "            html = resp.text\n",
        "            if is_placeholder(html):\n",
        "                raise RuntimeError('Placeholder')\n",
        "                \n",
        "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
        "            return url, html\n",
        "            \n",
        "        except Exception:\n",
        "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
        "            if attempt < MAX_FETCH_RETRIES:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                \n",
        "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
        "    return url, None\n",
        "\n",
        "def process_article(arg):\n",
        "    url, html = arg\n",
        "    if not html:\n",
        "        return None\n",
        "        \n",
        "    art = Article(url)\n",
        "    art.set_html(html)\n",
        "    \n",
        "    try:\n",
        "        art.parse()\n",
        "    except:\n",
        "        return None\n",
        "        \n",
        "    text = art.text or \"\"\n",
        "    title = (art.title or \"\").strip() or \"No title\"\n",
        "    \n",
        "    date, tm = safe_find_datetime(url, html)\n",
        "    \n",
        "    # Return combined data using dictionary unpacking\n",
        "    return {'publish_date': date, 'publish_time': tm,\n",
        "             'title': title, 'body_text': text, 'url': url}\n",
        "\n",
        "async def scrape_all(urls):\n",
        "    total = len(urls)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    \n",
        "    # Phase 1: Fetch HTML content from all URLs in parallel\n",
        "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
        "        # Create fetch tasks and run them through the thread pool\n",
        "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
        "                       for i, u in enumerate(urls)]\n",
        "        # Wait for all fetch tasks to complete\n",
        "        fetched = await asyncio.gather(*fetch_tasks)\n",
        "\n",
        "    # Phase 2: Process all fetched HTML content in parallel\n",
        "    records = []\n",
        "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
        "        # Submit processing tasks only for URLs with successful fetches\n",
        "        futures = {\n",
        "            proc_pool.submit(process_article, fr): fr[0]\n",
        "            for fr in fetched if fr[1]  # Skip URLs where HTML is None\n",
        "        }\n",
        "        \n",
        "        # Process results as they complete\n",
        "        for i, fut in enumerate(as_completed(futures), 1):\n",
        "            res = fut.result()\n",
        "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
        "            if res:\n",
        "                records.append(res)\n",
        "                \n",
        "    # Convert results to DataFrame\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# ——— Main entry point function ———\n",
        "def main(links):\n",
        "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
        "    return df\n",
        "\n",
        "# Execute the main function if this script is run directly\n",
        "if __name__ == '__main__':\n",
        "    df = main(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body_text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-05-27</td>\n",
              "      <td>10:33</td>\n",
              "      <td>US Consumer bounces back sharply in May By Inv...</td>\n",
              "      <td>Investing.com -- Consumer confidence rebounded...</td>\n",
              "      <td>https://www.investing.com/news/economy-news/us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-05-27</td>\n",
              "      <td>10:29</td>\n",
              "      <td>Eli Lilly expands pain pipeline with acquisiti...</td>\n",
              "      <td>Investing.com -- Eli Lilly and Company (NYSE: ...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-05-27</td>\n",
              "      <td>09:41</td>\n",
              "      <td>Lynx: Risk priced in, Nvidia could surprise to...</td>\n",
              "      <td>Investing.com -- Nvidia stock could break out ...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-05-27</td>\n",
              "      <td>09:16</td>\n",
              "      <td>Trump Media &amp; Technology Group Announces $2.5B...</td>\n",
              "      <td>Trump Media and Technology Group Corp. (Nasdaq...</td>\n",
              "      <td>https://www.investing.com/news/cryptocurrency-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-05-27</td>\n",
              "      <td>09:15</td>\n",
              "      <td>SoundHound shares jump as Piper Sandler starts...</td>\n",
              "      <td>Investing.com -- Piper Sandler has initiated c...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  publish_date publish_time  \\\n",
              "0   2025-05-27        10:33   \n",
              "1   2025-05-27        10:29   \n",
              "2   2025-05-27        09:41   \n",
              "3   2025-05-27        09:16   \n",
              "4   2025-05-27        09:15   \n",
              "\n",
              "                                               title  \\\n",
              "0  US Consumer bounces back sharply in May By Inv...   \n",
              "1  Eli Lilly expands pain pipeline with acquisiti...   \n",
              "2  Lynx: Risk priced in, Nvidia could surprise to...   \n",
              "3  Trump Media & Technology Group Announces $2.5B...   \n",
              "4  SoundHound shares jump as Piper Sandler starts...   \n",
              "\n",
              "                                           body_text  \\\n",
              "0  Investing.com -- Consumer confidence rebounded...   \n",
              "1  Investing.com -- Eli Lilly and Company (NYSE: ...   \n",
              "2  Investing.com -- Nvidia stock could break out ...   \n",
              "3  Trump Media and Technology Group Corp. (Nasdaq...   \n",
              "4  Investing.com -- Piper Sandler has initiated c...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.investing.com/news/economy-news/us...  \n",
              "1  https://www.investing.com/news/stock-market-ne...  \n",
              "2  https://www.investing.com/news/stock-market-ne...  \n",
              "3  https://www.investing.com/news/cryptocurrency-...  \n",
              "4  https://www.investing.com/news/stock-market-ne...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of articles with empty body_text: 0\n"
          ]
        }
      ],
      "source": [
        "# Count empty body_text entries\n",
        "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
        "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y-%m-%d %H-%M-%S\").strip().replace(' ', '_')\n",
        "df.to_csv(f\"Data/Headlines/{date_time}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2021-06-19 Stock news data collection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
