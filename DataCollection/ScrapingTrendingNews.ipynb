{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping from Investing.com\n",
        "- Scrape from trending news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TqhEjAwTgWKr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import cloudscraper\n",
        "from htmldate import find_date\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 20 links on page 1\n",
            "Only one page detected or no pagination found.\n"
          ]
        }
      ],
      "source": [
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# CONFIGURATION\n",
        "MAX_WORKERS = 10\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": (\n",
        "        \"text/html,application/xhtml+xml,application/xml;\"\n",
        "        \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
        "    ),\n",
        "    \"Referer\": \"https://www.investing.com/\",\n",
        "}\n",
        "\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows'}\n",
        ")\n",
        "\n",
        "def fetch_page(page: int):\n",
        "    # If page == 1, use default URL, else use pagination format\n",
        "    url = \"https://www.investing.com/news/most-popular-news\"\n",
        "    if page > 1:\n",
        "        url += f\"/{page}\"\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "            anchors = soup.select(\n",
        "                'ul[data-test=\"news-list\"] '\n",
        "                'li article a[data-test=\"article-title-link\"]'\n",
        "            )\n",
        "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_RETRIES:\n",
        "                backoff = 2 ** (attempt - 1) + random.random()\n",
        "                time.sleep(backoff)\n",
        "            else:\n",
        "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
        "    return []\n",
        "\n",
        "def robust_scrape(max_pages=10):\n",
        "    first = fetch_page(1)\n",
        "    PER_PAGE = len(first)\n",
        "    if PER_PAGE == 0:\n",
        "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies.\")\n",
        "\n",
        "    print(f\"Fetched {PER_PAGE} links on page 1\")\n",
        "\n",
        "    results = {1: first}\n",
        "\n",
        "    # Auto stop if only one page\n",
        "    if max_pages == 1 or PER_PAGE == 0:\n",
        "        print(\"Only one page detected or no pagination found.\")\n",
        "        return first\n",
        "\n",
        "    pages = list(range(2, max_pages + 1))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
        "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
        "        for fut in as_completed(futures):\n",
        "            p = futures[fut]\n",
        "            results[p] = fut.result()\n",
        "\n",
        "        for round in range(1, MAX_RETRIES + 1):\n",
        "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
        "            if not bad:\n",
        "                print(f\"All pages OK after {round - 1} retries\")\n",
        "                break\n",
        "            print(f\"Retry round {round} for pages: {bad}\")\n",
        "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
        "            for fut in as_completed(futures):\n",
        "                p = futures[fut]\n",
        "                results[p] = fut.result()\n",
        "        else:\n",
        "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
        "\n",
        "    total_fetched = sum(len(links) for links in results.values())\n",
        "    expected = PER_PAGE * max_pages\n",
        "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
        "\n",
        "    all_links = set(link for links in results.values() for link in links)\n",
        "    print(f\"Final: got {len(all_links)} unique URLs\")\n",
        "    return list(all_links)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    links = robust_scrape(max_pages=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://www.investing.com/news/cryptocurrency-news/bitcoin-price-today-steady-at-109k-amid-legislative-support-at-2025-conference-4066526'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "links[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Fetch][2/20][retry 1]\n",
            "[Fetch][5/20][ok]\n",
            "[Fetch][1/20][ok]\n",
            "[Fetch][11/20][ok]\n",
            "[Fetch][17/20][ok]\n",
            "[Fetch][16/20][ok]\n",
            "[Fetch][15/20][ok]\n",
            "[Fetch][18/20][ok]\n",
            "[Fetch][8/20][ok]\n",
            "[Fetch][14/20][ok]\n",
            "[Fetch][3/20][ok]\n",
            "[Fetch][19/20][ok]\n",
            "[Fetch][12/20][ok]\n",
            "[Fetch][7/20][ok]\n",
            "[Fetch][9/20][ok]\n",
            "[Fetch][4/20][ok]\n",
            "[Fetch][13/20][ok]\n",
            "[Fetch][6/20][ok]\n",
            "[Fetch][20/20][ok]\n",
            "[Fetch][2/20][ok]\n",
            "[Fetch][10/20][ok]\n",
            "[Process][1/20] https://www.investing.com/news/stock-market-news/wells-fargo-time-to-trim-us-small-caps-after-recent-rally-4067178\n",
            "[Process][2/20] https://www.investing.com/news/stock-market-news/barclays-sees-upside-in-stocks-as-rerisking-stays-subdued-4067114\n",
            "[Process][3/20] https://www.investing.com/news/stock-market-news/is-it-time-to-buysell-nvidia-ahead-of-earnings-heres-what-our-ai-model-says-4066634\n",
            "[Process][4/20] https://www.investing.com/news/cryptocurrency-news/bitcoin-price-today-steady-at-109k-amid-legislative-support-at-2025-conference-4066526\n",
            "[Process][5/20] https://www.investing.com/news/commodities-news/goldman-sachs-makes-the-strategic-case-for-gold-and-oil-in-longrun-portfolios-4068042\n",
            "[Process][6/20] https://www.investing.com/news/economy-news/nvidia-earnings-new-stellantis-ceo-japanese-bond-auction--whats-moving-markets-4066692\n",
            "[Process][7/20] https://www.investing.com/news/economy-news/trump-says-golden-dome-to-cost-canada-61-bln-but-free-if-it-gives-up-sovereignty-4066382\n",
            "[Process][8/20] https://www.investing.com/news/economy-news/elon-musk-must-face-lawsuit-over-his-role-in-doge-under-trump-judge-says-4066381\n",
            "[Process][9/20] https://www.investing.com/news/stock-market-news/us-stock-futures-jump-as-trump-delays-50-tariffs-against-the-eu-4063878\n",
            "[Process][10/20] https://www.investing.com/news/stock-market-news/asia-stocks-rise-tracking-wall-st-gains-tech-upbeat-before-nvidia-earnings-4066438\n",
            "[Process][11/20] https://www.investing.com/news/stock-market-news/tsx-up-slightly-ahead-of-nvidia-earnings-4068183\n",
            "[Process][12/20] https://www.investing.com/news/stock-market-news/jpm-highlights-top-6-european-investment-ideas-for-next-1218-months-4064930\n",
            "[Process][13/20] https://www.investing.com/news/cryptocurrency-news/jpmorgan-shifts-to-bullish-stance-on-crypto-amid-sector-tailwinds-4064834\n",
            "[Process][14/20] https://www.investing.com/news/stock-market-news/citi-sees-up-to-8-downside-for-stoxx-600-on-trump-tariff-risk-4064531\n",
            "[Process][15/20] https://www.investing.com/news/cryptocurrency-news/stanchart-sees-solana-price-rising-over-50-by-end-of-2025-4064655\n",
            "[Process][16/20] https://www.investing.com/news/stock-market-news/us-stock-futures-steady-with-nvidia-earnings-in-focus-4066366\n",
            "[Process][17/20] https://www.investing.com/news/stock-market-news/tempus-ai-stock-sinks-following-spruce-point-short-report-4068011\n",
            "[Process][18/20] https://www.investing.com/news/stock-market-news/this-is-what-matters-most-for-todays-nvidia-earnings-print-mizuho-4068117\n",
            "[Process][19/20] https://www.investing.com/news/assorted/estee-lauders-aveda-launches-in-amazon-premium-beauty-store-432SI-4067926\n",
            "[Process][20/20] https://www.investing.com/news/stock-market-news/gamestop-pulls-the-trigger-discloses-513-million-bitcoin-investment-4067988\n"
          ]
        }
      ],
      "source": [
        "nest_asyncio.apply()\n",
        "\n",
        "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
        "PROCESS_WORKERS = os.cpu_count() or 4\n",
        "MAX_FETCH_RETRIES = 3                      \n",
        "RETRY_DELAY = 1                             \n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "def is_placeholder(html: str) -> bool:\n",
        "    lower = html.lower() if html else \"\"\n",
        "    return (\n",
        "        'temporarily down for maintenance' in lower\n",
        "        or 'just a moment' in lower\n",
        "        or \"we're temporarily down\" in lower\n",
        "    )\n",
        "\n",
        "def safe_find_datetime(url, html_content=None):\n",
        "    try:\n",
        "        # Strategy 1: Use htmldate library to extract date from URL\n",
        "        dt = find_date(url)\n",
        "        if dt:\n",
        "            return dt, \"00:00\"  # Return with default time if date found\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    if html_content:\n",
        "        # Strategy 2: Look for American format with AM/PM\n",
        "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            try:\n",
        "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
        "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Strategy 3: Look for numeric dates with 24-hour time format\n",
        "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
        "        if m:\n",
        "            ds, ts = m.groups()\n",
        "            # Try both European and American date formats\n",
        "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
        "                try:\n",
        "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
        "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
        "                except:\n",
        "                    continue\n",
        "    \n",
        "    now = datetime.now()\n",
        "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
        "\n",
        "def fetch_html(url, idx, total):\n",
        "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
        "        try:\n",
        "            resp = scraper.get(url, timeout=30)\n",
        "            html = resp.text\n",
        "            if is_placeholder(html):\n",
        "                raise RuntimeError('Placeholder')\n",
        "                \n",
        "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
        "            return url, html\n",
        "            \n",
        "        except Exception:\n",
        "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
        "            if attempt < MAX_FETCH_RETRIES:\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                \n",
        "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
        "    return url, None\n",
        "\n",
        "def process_article(arg):\n",
        "    url, html = arg\n",
        "    if not html:\n",
        "        return None\n",
        "        \n",
        "    art = Article(url)\n",
        "    art.set_html(html)\n",
        "    \n",
        "    try:\n",
        "        art.parse()\n",
        "    except:\n",
        "        return None\n",
        "        \n",
        "    text = art.text or \"\"\n",
        "    title = (art.title or \"\").strip() or \"No title\"\n",
        "    \n",
        "    date, tm = safe_find_datetime(url, html)\n",
        "    \n",
        "    # Return combined data using dictionary unpacking\n",
        "    return {'publish_date': date, 'publish_time': tm,\n",
        "             'title': title, 'body_text': text, 'url': url}\n",
        "\n",
        "async def scrape_all(urls):\n",
        "    total = len(urls)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    \n",
        "    # Phase 1: Fetch HTML content from all URLs in parallel\n",
        "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
        "        # Create fetch tasks and run them through the thread pool\n",
        "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
        "                       for i, u in enumerate(urls)]\n",
        "        # Wait for all fetch tasks to complete\n",
        "        fetched = await asyncio.gather(*fetch_tasks)\n",
        "\n",
        "    # Phase 2: Process all fetched HTML content in parallel\n",
        "    records = []\n",
        "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
        "        # Submit processing tasks only for URLs with successful fetches\n",
        "        futures = {\n",
        "            proc_pool.submit(process_article, fr): fr[0]\n",
        "            for fr in fetched if fr[1]  # Skip URLs where HTML is None\n",
        "        }\n",
        "        \n",
        "        # Process results as they complete\n",
        "        for i, fut in enumerate(as_completed(futures), 1):\n",
        "            res = fut.result()\n",
        "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
        "            if res:\n",
        "                records.append(res)\n",
        "                \n",
        "    # Convert results to DataFrame\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# ——— Main entry point function ———\n",
        "def main(links):\n",
        "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
        "    return df\n",
        "\n",
        "# Execute the main function if this script is run directly\n",
        "if __name__ == '__main__':\n",
        "    df = main(links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body_text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-05-28</td>\n",
              "      <td>10:26</td>\n",
              "      <td>TSX up slightly ahead of NVIDIA earnings By In...</td>\n",
              "      <td>Investing.com -- Canada’s main stock index was...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-05-28</td>\n",
              "      <td>09:58</td>\n",
              "      <td>This is what matters most for today’s Nvidia e...</td>\n",
              "      <td>Investing.com -- As Nvidia (NASDAQ: ) gears up...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-05-28</td>\n",
              "      <td>09:36</td>\n",
              "      <td>Goldman Sachs makes the strategic case for gol...</td>\n",
              "      <td>Investing.com -- Goldman Sachs made the case f...</td>\n",
              "      <td>https://www.investing.com/news/commodities-new...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-05-28</td>\n",
              "      <td>09:22</td>\n",
              "      <td>Tempus AI stock sinks following Spruce Point s...</td>\n",
              "      <td>Investing.com -- Shares of Tempus AI (NASDAQ: ...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-05-28</td>\n",
              "      <td>09:17</td>\n",
              "      <td>GameStop pulls the trigger, discloses $513 mil...</td>\n",
              "      <td>Investing.com -- GameStop Corp (NYSE: ) shares...</td>\n",
              "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  publish_date publish_time  \\\n",
              "0   2025-05-28        10:26   \n",
              "1   2025-05-28        09:58   \n",
              "2   2025-05-28        09:36   \n",
              "3   2025-05-28        09:22   \n",
              "4   2025-05-28        09:17   \n",
              "\n",
              "                                               title  \\\n",
              "0  TSX up slightly ahead of NVIDIA earnings By In...   \n",
              "1  This is what matters most for today’s Nvidia e...   \n",
              "2  Goldman Sachs makes the strategic case for gol...   \n",
              "3  Tempus AI stock sinks following Spruce Point s...   \n",
              "4  GameStop pulls the trigger, discloses $513 mil...   \n",
              "\n",
              "                                           body_text  \\\n",
              "0  Investing.com -- Canada’s main stock index was...   \n",
              "1  Investing.com -- As Nvidia (NASDAQ: ) gears up...   \n",
              "2  Investing.com -- Goldman Sachs made the case f...   \n",
              "3  Investing.com -- Shares of Tempus AI (NASDAQ: ...   \n",
              "4  Investing.com -- GameStop Corp (NYSE: ) shares...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.investing.com/news/stock-market-ne...  \n",
              "1  https://www.investing.com/news/stock-market-ne...  \n",
              "2  https://www.investing.com/news/commodities-new...  \n",
              "3  https://www.investing.com/news/stock-market-ne...  \n",
              "4  https://www.investing.com/news/stock-market-ne...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of articles with empty body_text: 0\n"
          ]
        }
      ],
      "source": [
        "# Count empty body_text entries\n",
        "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
        "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "date_time = now.strftime(\"%Y-%m-%d %H-%M-%S\").strip().replace(' ', '_')\n",
        "df.to_csv(f\"Data/Trending_News/{date_time}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2021-06-19 Stock news data collection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
