{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f332e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import cloudscraper\n",
    "from htmldate import find_date\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c058741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 20 links on page 1\n",
      "Only one page detected or no pagination found.\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAX_WORKERS = 10\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": (\n",
    "        \"text/html,application/xhtml+xml,application/xml;\"\n",
    "        \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
    "    ),\n",
    "    \"Referer\": \"https://www.investing.com/\",\n",
    "}\n",
    "\n",
    "scraper = cloudscraper.create_scraper(\n",
    "    browser={'browser': 'chrome', 'platform': 'windows'}\n",
    ")\n",
    "\n",
    "def fetch_page(page: int):\n",
    "    url = \"https://www.investing.com/news/most-popular-news\"\n",
    "    if page > 1:\n",
    "        url += f\"/{page}\"\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = scraper.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "            anchors = soup.select(\n",
    "                'ul[data-test=\"news-list\"] '\n",
    "                'li article a[data-test=\"article-title-link\"]'\n",
    "            )\n",
    "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff = 2 ** (attempt - 1) + random.random()\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
    "    return []\n",
    "\n",
    "def robust_scrape(max_pages=10):\n",
    "    first = fetch_page(1)\n",
    "    PER_PAGE = len(first)\n",
    "    if PER_PAGE == 0:\n",
    "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies.\")\n",
    "\n",
    "    print(f\"Fetched {PER_PAGE} links on page 1\")\n",
    "\n",
    "    results = {1: first}\n",
    "\n",
    "    if max_pages == 1 or PER_PAGE == 0:\n",
    "        print(\"Only one page detected or no pagination found.\")\n",
    "        return first\n",
    "\n",
    "    pages = list(range(2, max_pages + 1))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
    "        for fut in as_completed(futures):\n",
    "            p = futures[fut]\n",
    "            results[p] = fut.result()\n",
    "\n",
    "        for round in range(1, MAX_RETRIES + 1):\n",
    "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
    "            if not bad:\n",
    "                print(f\"All pages OK after {round - 1} retries\")\n",
    "                break\n",
    "            print(f\"Retry round {round} for pages: {bad}\")\n",
    "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
    "            for fut in as_completed(futures):\n",
    "                p = futures[fut]\n",
    "                results[p] = fut.result()\n",
    "        else:\n",
    "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
    "\n",
    "    total_fetched = sum(len(links) for links in results.values())\n",
    "    expected = PER_PAGE * max_pages\n",
    "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
    "\n",
    "    all_links = set(link for links in results.values() for link in links)\n",
    "    print(f\"Final: got {len(all_links)} unique URLs\")\n",
    "    return list(all_links)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = robust_scrape(max_pages=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19450aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fetch][1/20][ok]\n",
      "[Fetch][17/20][ok]\n",
      "[Fetch][19/20][ok]\n",
      "[Fetch][8/20][ok]\n",
      "[Fetch][6/20][ok]\n",
      "[Fetch][2/20][ok]\n",
      "[Fetch][4/20][ok]\n",
      "[Fetch][9/20][ok]\n",
      "[Fetch][13/20][ok]\n",
      "[Fetch][3/20][ok]\n",
      "[Fetch][5/20][ok]\n",
      "[Fetch][15/20][ok]\n",
      "[Fetch][20/20][ok]\n",
      "[Fetch][10/20][ok]\n",
      "[Fetch][18/20][ok]\n",
      "[Fetch][16/20][ok]\n",
      "[Fetch][14/20][ok]\n",
      "[Fetch][12/20][ok]\n",
      "[Fetch][7/20][ok]\n",
      "[Fetch][11/20][ok]\n",
      "[Process][1/20] https://www.investing.com/news/politics-news/musk-calls-trumps-big-beautiful-bill-porkfilled-4078918\n",
      "[Process][2/20] https://www.investing.com/news/stock-market-news/piper-sandler-is-tesla-the-only-us-automaker-with-a-plan-to-bypass-china-4077712\n",
      "[Process][3/20] https://www.investing.com/news/economy-news/wolfe-trump-trump-budget-bill-slightly-worse-for-deficits-than-current-policy-4077823\n",
      "[Process][4/20] https://www.investing.com/news/cryptocurrency-news/bitcoin-price-today-recovers-to-105k-but-trade-economic-jitters-limit-upside-4076970\n",
      "[Process][5/20] https://www.investing.com/news/stock-market-news/nasdaq-outperformance-how-long-can-the-rally-last-4077941\n",
      "[Process][6/20] https://www.investing.com/news/stock-market-news/these-are-the-key-winners-in-americas-aidriven-energy-surge-morgan-stanley-4077466\n",
      "[Process][7/20] https://www.investing.com/news/stock-market-news/asia-stocks-rise-on-hopes-of-trumpxi-talk-skorea-rallies-on-election-results-4079417\n",
      "[Process][8/20] https://www.investing.com/news/stock-market-news/us-stock-futures-edge-lower-with-focus-on-trade-tariffs-economic-cues-4076911\n",
      "[Process][9/20] https://www.investing.com/news/economy-news/eu-yet-to-receive-us-letter-demanding-best-trade-negotiation-offers-4077486\n",
      "[Process][10/20] https://www.investing.com/news/stock-market-news/any-pullback-in-stocks-would-be-brief-hsbcs-kettner-4077528\n",
      "[Process][11/20] https://www.investing.com/news/economy-news/oecd-slashes-global-growth-forecast-flags-impact-of-tariff-strife-on-us-economy-4077300\n",
      "[Process][12/20] https://www.investing.com/news/stock-market-news/deutsche-bank-raises-yearend-outlook-for-sp-500-4077161\n",
      "[Process][13/20] https://www.investing.com/news/stock-market-news/why-bernstein-thinks-boeing-stock-is-the-best-idea-in-aerospace--defense-4077525\n",
      "[Process][14/20] https://www.investing.com/news/economy-news/futures-lower-jolts-ahead-tsmc-ceo-on-tariffs--whats-moving-markets-4077101\n",
      "[Process][15/20] https://www.investing.com/news/stock-market-news/dutch-aex-slips-as-farright-leader-leaves-coalition-toppling-government-4077204\n",
      "[Process][16/20] https://www.investing.com/news/stock-market-news/after-notching-four-100-wins-here-are-our-ais-global-picks-for-june-4074772\n",
      "[Process][17/20] https://www.investing.com/news/commodities-news/gold-prices-steady-with-focus-on-trumpxi-talk-geopolitics-4079457\n",
      "[Process][18/20] https://www.investing.com/news/cryptocurrency-news/trumps-truthsocial-progresses-towards-bitcoin-etf-launch-filing-shows-4079423\n",
      "[Process][19/20] https://www.investing.com/news/cryptocurrency-news/bitcoin-price-today-flat-at-105k-amid-trade-uncertainty-trump-etf-progresses-4079466\n",
      "[Process][20/20] https://www.investing.com/news/stock-market-news/us-stock-futures-steady-with-trumpxi-call-higher-tariffs-in-focus-4079387\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
    "PROCESS_WORKERS = os.cpu_count() or 4\n",
    "MAX_FETCH_RETRIES = 3                      \n",
    "RETRY_DELAY = 1                             \n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "def is_placeholder(html: str) -> bool:\n",
    "    lower = html.lower() if html else \"\"\n",
    "    return (\n",
    "        'temporarily down for maintenance' in lower\n",
    "        or 'just a moment' in lower\n",
    "        or \"we're temporarily down\" in lower\n",
    "    )\n",
    "\n",
    "def safe_find_datetime(url, html_content=None):\n",
    "    try:\n",
    "        dt = find_date(url)\n",
    "        if dt:\n",
    "            return dt, \"00:00\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if html_content:\n",
    "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            try:\n",
    "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
    "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
    "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
    "\n",
    "def fetch_html(url, idx, total):\n",
    "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            resp = scraper.get(url, timeout=30)\n",
    "            html = resp.text\n",
    "            if is_placeholder(html):\n",
    "                raise RuntimeError('Placeholder')\n",
    "                \n",
    "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
    "            return url, html\n",
    "            \n",
    "        except Exception:\n",
    "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
    "            if attempt < MAX_FETCH_RETRIES:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                \n",
    "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
    "    return url, None\n",
    "\n",
    "def process_article(arg):\n",
    "    url, html = arg\n",
    "    if not html:\n",
    "        return None\n",
    "        \n",
    "    art = Article(url)\n",
    "    art.set_html(html)\n",
    "    \n",
    "    try:\n",
    "        art.parse()\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    text = art.text or \"\"\n",
    "    title = (art.title or \"\").strip() or \"No title\"\n",
    "    \n",
    "    date, tm = safe_find_datetime(url, html)\n",
    "    \n",
    "    return {'publish_date': date, 'publish_time': tm,\n",
    "             'title': title, 'body_text': text, 'url': url}\n",
    "\n",
    "async def scrape_all(urls):\n",
    "    total = len(urls)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
    "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
    "                       for i, u in enumerate(urls)]\n",
    "        fetched = await asyncio.gather(*fetch_tasks)\n",
    "\n",
    "    records = []\n",
    "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
    "        futures = {\n",
    "            proc_pool.submit(process_article, fr): fr[0]\n",
    "            for fr in fetched if fr[1]\n",
    "        }\n",
    "        \n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            res = fut.result()\n",
    "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
    "            if res:\n",
    "                records.append(res)\n",
    "            \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def main(links):\n",
    "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3d41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357c3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with empty body_text: 0\n"
     ]
    }
   ],
   "source": [
    "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
    "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88b2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../SentimentAnalysis/GPT/secret.env\")\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key is None:\n",
    "    print(\"Error: GEMINI_API_KEY not found in .env file or environment variables.\")\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88143a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = genai.GenerationConfig(\n",
    "        temperature=0,\n",
    ")\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-preview-04-17\", generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eae589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "def is_retryable(e) -> bool:\n",
    "    if retry.if_transient_error(e):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ClientError) and e.code == 429):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ServerError) and e.code == 503):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "@retry.Retry(predicate=is_retryable)\n",
    "def generate_content_with_rate_limit(prompt):\n",
    "  return model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4e5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a financial news analyst specializing in stock market impact. Your task is to analyze the provided news article, summarize its core content concisely, determine its sentiment (positive, negative, or neutral), and assess its importance to the specified stock.\n",
    "\n",
    "Here is the news from stock [STOCK] title and body:\n",
    "---\n",
    "[TITLE]\n",
    "---\n",
    "[BODY]\n",
    "---\n",
    "\n",
    "Please provide your analysis in the following format (Don't forget to make space between the sections as shown):\n",
    "\n",
    "**Sentiment:**\n",
    "[Positive / Negative / Neutral]\n",
    "\n",
    "**Summary:**\n",
    "[Your concise summary of the article, typically 2-3 sentences.]\n",
    "\n",
    "**Reasoning for Sentiment:**\n",
    "[Brief explanation (1-2 sentences) of why you categorized the sentiment as such, referencing key points or tone from the article.]\n",
    "\n",
    "**Importance to Stock [STOCK]:**\n",
    "[1-5, where 1 is minimal importance and 5 is very high importance.Answer in 1-5 only, no explanation.] (Answer only in number 1-5)\n",
    "\n",
    "**Reasoning for Importance:**\n",
    "[Brief explanation (1-2 sentences) of why you assigned this importance score, referencing specific details from the article that would impact the stock.]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14935da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [02:07<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "    current_stock = row.get(\"ticker\", \"news\")\n",
    "\n",
    "    filled_prompt = prompt.replace(\"[STOCK]\", current_stock)\n",
    "    filled_prompt = filled_prompt.replace(\"[TITLE]\", row[\"title\"])\n",
    "    filled_prompt = filled_prompt.replace(\"[BODY]\", row[\"body_text\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            predicted.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            predicted.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        predicted.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            predicted.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             predicted.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            predicted.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ba4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted = np.array(predicted)\n",
    "df[\"predicted\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845ac458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[1].strip() if len(x.split(\"\\n\")) > 1 else None)\n",
    "df[\"importance\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[10].strip() if len(x.split(\"\\n\")) > 10 else None)\n",
    "df[\"summary\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[4].strip() if len(x.split(\"\\n\")) > 4 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c4c9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentiment'].isin(['Positive', 'Negative', 'Neutral'])]\n",
    "df = df[df['importance'].isin(['1', '2', '3', '4', '5'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce24deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\", generation_config=generation_config)\n",
    "prompt = \"\"\"Translate the following English sentence to Thai. Do not translate proper nouns, company names, product names, abbreviations, or technical terms — keep them in English. Do not provide any explanation, just the translation.\n",
    "[TEXT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e91f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [01:25<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "translate = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "\n",
    "    filled_prompt = prompt.replace(\"[TEXT]\", row[\"summary\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            translate.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            translate.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        translate.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            translate.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             translate.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            translate.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b83a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER=\"news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a74b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "if 'predicted' in df.columns:\n",
    "    df.drop(columns=['predicted'], inplace=True)\n",
    "if 'body_text' in df.columns:\n",
    "    df.drop(columns=['body_text'], inplace=True)\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d %H-%M\").strip().replace(' ', '_')\n",
    "filename = f\"Gemini_{TICKER}_{date_time}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29147a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"translate\"] = translate\n",
    "df.to_csv(f\"Data/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cd6730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to be passed to Streamlit: ../CompletePipeline/Data/Gemini_news_2025-06-04_14-08.csv\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.0.2:8501\u001b[0m\n",
      "\u001b[0m\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "data_filepath_for_streamlit = os.path.join(\"..\", \"CompletePipeline\", \"Data\", filename)\n",
    "\n",
    "print(f\"Path to be passed to Streamlit: {data_filepath_for_streamlit}\")\n",
    "\n",
    "!streamlit run ../Visualization/news_app.py \"{data_filepath_for_streamlit}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
