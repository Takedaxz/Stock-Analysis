{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af71cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import cloudscraper\n",
    "from htmldate import find_date\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b193b940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 10 links per page, expecting 20 total\n",
      "All pages OK after 0 retries\n",
      "Total links fetched (including duplicates): 20 (expected 20)\n",
      "Final: got 20 unique URLs (expected 20)\n"
     ]
    }
   ],
   "source": [
    "COMPANY     = \"google-inc\"   \n",
    "MAX_PAGE    = 2\n",
    "MAX_WORKERS = 50              \n",
    "MAX_RETRIES = 5              \n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": (\n",
    "        \"text/html,application/xhtml+xml,application/xml;\"\n",
    "        \"q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
    "    ),\n",
    "    \"Referer\": \"https://www.investing.com/\",\n",
    "}\n",
    "\n",
    "scraper = cloudscraper.create_scraper(\n",
    "    browser={'browser': 'chrome', 'platform': 'windows'}\n",
    ")\n",
    "\n",
    "def fetch_page(page: int):\n",
    "    global ticker\n",
    "    #Equity\n",
    "    url = f\"https://www.investing.com/equities/{COMPANY}-news/{page}\"\n",
    "    #Index\n",
    "    #url = f\"https://www.investing.com/indices/{COMPANY}-news/{page}\"\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = scraper.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            h1_tag = soup.find('h1', class_='mb-2.5') # Use a specific class or combination of classes for robustness\n",
    "            full_text = h1_tag.text.strip()\n",
    "            match = re.search(r'\\(([^)]+)\\)', full_text)\n",
    "            ticker = match.group(1)\n",
    "                    \n",
    "            anchors = soup.select(\n",
    "                'ul[data-test=\"news-list\"] '\n",
    "                'li article a[data-test=\"article-title-link\"]'\n",
    "            )\n",
    "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff = 2 ** (attempt - 1) + random.random()\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
    "    return []\n",
    "\n",
    "def robust_scrape():\n",
    "    first = fetch_page(1)\n",
    "    PER_PAGE = len(first)\n",
    "    if PER_PAGE == 0:\n",
    "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies and try again.\")\n",
    "    print(f\"Detected {PER_PAGE} links per page, expecting {PER_PAGE * MAX_PAGE} total\")\n",
    "\n",
    "    results = {1: first}\n",
    "    pages = list(range(2, MAX_PAGE + 1))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
    "        for fut in as_completed(futures):\n",
    "            p = futures[fut]\n",
    "            results[p] = fut.result()\n",
    "\n",
    "        for round in range(1, MAX_RETRIES + 1):\n",
    "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
    "            if not bad:\n",
    "                print(f\"All pages OK after {round-1} retries\")\n",
    "                break\n",
    "            print(f\"Retry round {round} for pages: {bad}\")\n",
    "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
    "            for fut in as_completed(futures):\n",
    "                p = futures[fut]\n",
    "                results[p] = fut.result()\n",
    "        else:\n",
    "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
    "\n",
    "    total_fetched = sum(len(links) for links in results.values())\n",
    "    expected = PER_PAGE * MAX_PAGE\n",
    "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
    "\n",
    "    all_links = set(link for links in results.values() for link in links)\n",
    "    print(f\"Final: got {len(all_links)} unique URLs (expected {expected})\")\n",
    "    return list(all_links)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = robust_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf873f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fetch][3/20][retry 1]\n",
      "[Fetch][11/20][retry 1]\n",
      "[Fetch][6/20][retry 1]\n",
      "[Fetch][17/20][retry 1]\n",
      "[Fetch][12/20][retry 1]\n",
      "[Fetch][14/20][retry 1]\n",
      "[Fetch][18/20][retry 1]\n",
      "[Fetch][16/20][retry 1]\n",
      "[Fetch][2/20][retry 1]\n",
      "[Fetch][13/20][retry 1]\n",
      "[Fetch][1/20][retry 1]\n",
      "[Fetch][10/20][retry 1]\n",
      "[Fetch][15/20][retry 1]\n",
      "[Fetch][7/20][retry 1]\n",
      "[Fetch][8/20][retry 1]\n",
      "[Fetch][9/20][retry 1]\n",
      "[Fetch][4/20][retry 1]\n",
      "[Fetch][5/20][retry 1]\n",
      "[Fetch][20/20][ok]\n",
      "[Fetch][19/20][ok]\n",
      "[Fetch][18/20][ok]\n",
      "[Fetch][13/20][ok]\n",
      "[Fetch][16/20][ok]\n",
      "[Fetch][3/20][ok]\n",
      "[Fetch][17/20][ok]\n",
      "[Fetch][2/20][ok]\n",
      "[Fetch][15/20][ok]\n",
      "[Fetch][11/20][ok]\n",
      "[Fetch][10/20][ok]\n",
      "[Fetch][8/20][ok]\n",
      "[Fetch][7/20][ok]\n",
      "[Fetch][6/20][ok]\n",
      "[Fetch][9/20][ok]\n",
      "[Fetch][4/20][ok]\n",
      "[Fetch][12/20][ok]\n",
      "[Fetch][1/20][ok]\n",
      "[Fetch][14/20][ok]\n",
      "[Fetch][5/20][ok]\n",
      "[Process][1/20] https://www.investing.com/news/pro/raymond-james-maintains-lululemon-athletica-at-market-perform-4077935\n",
      "[Process][2/20] https://www.investing.com/news/pro/bofa-securities-maintains-alphabet-a-at-buy-with-a-price-target-of-20000-4076601\n",
      "[Process][3/20] https://www.investing.com/news/stock-market-news/google-says-it-will-appeal-online-search-antitrust-decision-4074869\n",
      "[Process][4/20] https://www.investing.com/news/stock-market-news/snowflake-expands-ai-capabilities-with-250-million-crunchy-data-deal--wsj-4076577\n",
      "[Process][5/20] https://www.investing.com/news/stock-market-news/alphabet-stock-falls-on-monopoly-case-uncertainties-4075693\n",
      "[Process][6/20] https://www.investing.com/news/stock-market-news/forest-blizzard-vs-fancy-bear--cyber-companies-hope-to-untangle-weird-hacker-nicknames-4076391\n",
      "[Process][7/20] https://www.investing.com/news/stock-market-news/dutch-aex-slips-as-farright-leader-leaves-coalition-toppling-government-4077204\n",
      "[Process][8/20] https://www.investing.com/news/transcripts/veritone-at-53rd-jpmorgan-conference-refocusing-on-ai-and-data-refinery-93CH-4048647\n",
      "[Process][9/20] https://www.investing.com/news/pro/truist-securities-maintains-alphabet-a-at-buy-with-a-price-target-of-20000-4078144\n",
      "[Process][10/20] https://www.investing.com/news/stock-market-news/ai-vibe-coding-startups-burst-onto-scene-with-skyhigh-valuations-4077418\n",
      "[Process][11/20] https://www.investing.com/news/pro/jefferies-maintains-applovin-at-buy-with-a-price-target-of-53000-4076685\n",
      "[Process][12/20] https://www.investing.com/news/stock-market-news/meta-aims-to-fully-automate-advertising-with-ai-by-2026-wsj-reports-4075414\n",
      "[Process][13/20] https://www.investing.com/news/pro/alphabet-shares-edge-lower-on-reports-of-apple-interest-in-perplexity-pact-432SI-4078017\n",
      "[Process][14/20] https://www.investing.com/news/stock-market-news/apple-stock-bofa-breaks-down-key-topics-in-bull-vs-bear-market-debate-4075894\n",
      "[Process][15/20] https://www.investing.com/news/stock-market-news/tesla-and-magnificent-seven-stocks-dip-in-premarket-amid-trade-tensions-93CH-4074950\n",
      "[Process][16/20] https://www.investing.com/news/analyst-ratings/bofa-securities-maintains-buy-rating-on-alphabet-stock-amid-samsung-talks-93CH-4076764\n",
      "[Process][17/20] https://www.investing.com/news/pro/jpmorgan-maintains-apple-at-overweight-with-a-price-target-of-24000-4077382\n",
      "[Process][18/20] https://www.investing.com/news/stock-market-news/apple-challenges-unreasonable-eu-order-to-open-up-to-rivals-4076289\n",
      "[Process][19/20] https://www.investing.com/news/stock-market-news/google-to-spend-500-million-revamping-compliance-in-shareholder-settlement-4076188\n",
      "[Process][20/20] https://www.investing.com/news/stock-market-news/nasdaq-outperformance-how-long-can-the-rally-last-4077941\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
    "PROCESS_WORKERS = os.cpu_count() or 4\n",
    "MAX_FETCH_RETRIES = 3                      \n",
    "RETRY_DELAY = 1                             \n",
    "TICKER = ticker\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "def is_placeholder(html: str) -> bool:\n",
    "    lower = html.lower() if html else \"\"\n",
    "    return (\n",
    "        'temporarily down for maintenance' in lower\n",
    "        or 'just a moment' in lower\n",
    "        or \"we're temporarily down\" in lower\n",
    "    )\n",
    "\n",
    "def safe_find_datetime(url, html_content=None):\n",
    "    try:\n",
    "        # Strategy 1: Use htmldate library to extract date from URL\n",
    "        dt = find_date(url)\n",
    "        if dt:\n",
    "            return dt, \"00:00\"  # Return with default time if date found\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if html_content:\n",
    "        # Strategy 2: Look for American format with AM/PM\n",
    "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            try:\n",
    "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
    "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Strategy 3: Look for numeric dates with 24-hour time format\n",
    "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            # Try both European and American date formats\n",
    "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
    "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
    "\n",
    "def fetch_html(url, idx, total):\n",
    "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            resp = scraper.get(url, timeout=30)\n",
    "            html = resp.text\n",
    "            if is_placeholder(html):\n",
    "                raise RuntimeError('Placeholder')\n",
    "                \n",
    "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
    "            return url, html\n",
    "            \n",
    "        except Exception:\n",
    "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
    "            if attempt < MAX_FETCH_RETRIES:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                \n",
    "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
    "    return url, None\n",
    "\n",
    "def process_article(arg):\n",
    "    url, html = arg\n",
    "    if not html:\n",
    "        return None\n",
    "        \n",
    "    art = Article(url)\n",
    "    art.set_html(html)\n",
    "    \n",
    "    try:\n",
    "        art.parse()\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    text = art.text or \"\"\n",
    "    title = (art.title or \"\").strip() or \"No title\"\n",
    "    \n",
    "    date, tm = safe_find_datetime(url, html)\n",
    "    \n",
    "    # Return combined data using dictionary unpacking\n",
    "    return {'ticker': TICKER, 'publish_date': date, 'publish_time': tm,\n",
    "             'title': title, 'body_text': text, 'url': url}\n",
    "\n",
    "async def scrape_all(urls):\n",
    "    total = len(urls)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Phase 1: Fetch HTML content from all URLs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
    "        # Create fetch tasks and run them through the thread pool\n",
    "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
    "                       for i, u in enumerate(urls)]\n",
    "        # Wait for all fetch tasks to complete\n",
    "        fetched = await asyncio.gather(*fetch_tasks)\n",
    "\n",
    "    # Phase 2: Process all fetched HTML content in parallel\n",
    "    records = []\n",
    "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
    "        # Submit processing tasks only for URLs with successful fetches\n",
    "        futures = {\n",
    "            proc_pool.submit(process_article, fr): fr[0]\n",
    "            for fr in fetched if fr[1]  # Skip URLs where HTML is None\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            res = fut.result()\n",
    "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
    "            if res:\n",
    "                records.append(res)\n",
    "                \n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# ——— Main entry point function ———\n",
    "def main(links):\n",
    "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
    "    return df\n",
    "\n",
    "# Execute the main function if this script is run directly\n",
    "if __name__ == '__main__':\n",
    "    df = main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8556e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d94bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with empty body_text: 0\n"
     ]
    }
   ],
   "source": [
    "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
    "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52396323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../SentimentAnalysis/GPT/secret.env\")\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key is None:\n",
    "    print(\"Error: GEMINI_API_KEY not found in .env file or environment variables.\")\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = genai.GenerationConfig(\n",
    "        temperature=0,\n",
    ")\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-preview-04-17\", generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd63ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "def is_retryable(e) -> bool:\n",
    "    if retry.if_transient_error(e):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ClientError) and e.code == 429):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ServerError) and e.code == 503):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "@retry.Retry(predicate=is_retryable)\n",
    "def generate_content_with_rate_limit(prompt):\n",
    "  return model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf008dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a financial news analyst specializing in stock market impact. Your task is to analyze the provided news article, summarize its core content concisely, determine its sentiment (positive, negative, or neutral), and assess its importance to the specified stock.\n",
    "\n",
    "Here is the news from stock [STOCK] title and body:\n",
    "---\n",
    "[TITLE]\n",
    "---\n",
    "[BODY]\n",
    "---\n",
    "\n",
    "Please provide your analysis in the following format (Don't forget to make space between the sections as shown):\n",
    "\n",
    "**Sentiment:**\n",
    "[Positive / Negative / Neutral]\n",
    "\n",
    "**Summary:**\n",
    "[Your concise summary of the article, typically 2-3 sentences.]\n",
    "\n",
    "**Reasoning for Sentiment:**\n",
    "[Brief explanation (1-2 sentences) of why you categorized the sentiment as such, referencing key points or tone from the article.]\n",
    "\n",
    "**Importance to Stock [STOCK]:**\n",
    "[1-5, where 1 is minimal importance and 5 is very high importance.Answer in 1-5 only, no explanation.] (Answer only in number 1-5)\n",
    "\n",
    "**Reasoning for Importance:**\n",
    "[Brief explanation (1-2 sentences) of why you assigned this importance score, referencing specific details from the article that would impact the stock.]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af69c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [02:04<00:00,  6.21s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "    current_stock = row.get(\"ticker\", \"news\")\n",
    "\n",
    "    filled_prompt = prompt.replace(\"[STOCK]\", current_stock)\n",
    "    filled_prompt = filled_prompt.replace(\"[TITLE]\", row[\"title\"])\n",
    "    filled_prompt = filled_prompt.replace(\"[BODY]\", row[\"body_text\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            predicted.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            predicted.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        predicted.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            predicted.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             predicted.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            predicted.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b6b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted = np.array(predicted)\n",
    "df[\"predicted\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df145704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[1].strip() if len(x.split(\"\\n\")) > 1 else None)\n",
    "df[\"importance\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[10].strip() if len(x.split(\"\\n\")) > 10 else None)\n",
    "df[\"summary\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[4].strip() if len(x.split(\"\\n\")) > 4 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8ae451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentiment'].isin(['Positive', 'Negative', 'Neutral'])]\n",
    "df = df[df['importance'].isin(['1', '2', '3', '4', '5'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c97e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\", generation_config=generation_config)\n",
    "prompt = \"\"\"Translate the following English sentence to Thai. Do not translate proper nouns, company names, product names, abbreviations, or technical terms — keep them in English. Do not provide any explanation, just the translation.\n",
    "[TEXT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "027e30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [25:20<00:00, 76.00s/it] \n"
     ]
    }
   ],
   "source": [
    "translate = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "\n",
    "    filled_prompt = prompt.replace(\"[TEXT]\", row[\"summary\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            translate.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            translate.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        translate.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            translate.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             translate.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            translate.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b227831",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER = current_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07fcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "if 'predicted' in df.columns:\n",
    "    df.drop(columns=['predicted'], inplace=True)\n",
    "if 'body_text' in df.columns:\n",
    "    df.drop(columns=['body_text'], inplace=True)\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d %H-%M\").strip().replace(' ', '_')\n",
    "filename = f\"Gemini_{TICKER}_{date_time}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbbd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"translate\"] = translate\n",
    "df.to_csv(f\"Data/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2445b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to be passed to Streamlit: ../CompletePipeline/Data/Gemini_GOOGL_2025-06-03_21-33.csv\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.0.2:8501\u001b[0m\n",
      "\u001b[0m\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "data_filepath_for_streamlit = os.path.join(\"..\", \"CompletePipeline\", \"Data\", filename)\n",
    "\n",
    "print(f\"Path to be passed to Streamlit: {data_filepath_for_streamlit}\")\n",
    "\n",
    "!streamlit run ../Visualization/stock_app.py \"{data_filepath_for_streamlit}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
