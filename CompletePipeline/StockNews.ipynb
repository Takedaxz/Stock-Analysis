{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af71cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import asyncio\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor,  as_completed\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import cloudscraper\n",
    "from htmldate import find_date\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72e951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc85741f9944823aafcf311c4b29ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Company:', layout=Layout(width='300px'), options=('Tesla', 'NVIDIA', 'Apple', 'Microsoft…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "companies = {\n",
    "    \"Tesla\": \"tesla-motors\",\n",
    "    \"NVIDIA\": \"nvidia-corp\",\n",
    "    \"Apple\": \"apple-computer-inc\",\n",
    "    \"Microsoft\": \"microsoft-corp\",\n",
    "    \"Amazon\": \"amazon-com-inc\",\n",
    "    \"Google\": \"google-inc\",\n",
    "    \"Meta\": \"facebook-inc\",\n",
    "    \"Netflix\": \"netflix,-inc.\",\n",
    "    \"AMD\": \"advanced-micro-devices\",\n",
    "}\n",
    "\n",
    "# Create a dropdown widget\n",
    "company_dropdown = widgets.Dropdown(\n",
    "    options=list(companies.keys()),\n",
    "    value='Tesla',\n",
    "    description='Company:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '300px'}\n",
    ")\n",
    "\n",
    "display(company_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b193b940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 10 links per page, expecting 20 total\n",
      "All pages OK after 0 retries\n",
      "Total links fetched (including duplicates): 20 (expected 20)\n",
      "Final: got 20 unique URLs (expected 20)\n"
     ]
    }
   ],
   "source": [
    "COMPANY = companies[company_dropdown.value]\n",
    "MAX_PAGE    = 2\n",
    "MAX_WORKERS = 50              \n",
    "MAX_RETRIES = 8              \n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Referer\": \"https://www.investing.com/\",\n",
    "    \"DNT\": \"1\"\n",
    "}\n",
    "\n",
    "scraper = cloudscraper.create_scraper(\n",
    "    browser={\n",
    "        'browser': 'chrome',\n",
    "        'platform': 'darwin',\n",
    "        'mobile': False\n",
    "    },\n",
    "    delay=2\n",
    ")\n",
    "\n",
    "def fetch_page(page: int):\n",
    "    global ticker\n",
    "    #Equity\n",
    "    url = f\"https://www.investing.com/equities/{COMPANY}-news/{page}\"\n",
    "    #Index\n",
    "    #url = f\"https://www.investing.com/indices/nq-100-news/{page}\"\n",
    "    #url = f\"https://www.investing.com/indices/us-spx-500-news/{page}\"\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = scraper.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"lxml\")\n",
    "            h1_tag = soup.find('h1', class_='mb-2.5') # Use a specific class or combination of classes for robustness\n",
    "            full_text = h1_tag.text.strip()\n",
    "            match = re.search(r'\\(([^)]+)\\)', full_text)\n",
    "            ticker = match.group(1)\n",
    "                    \n",
    "            anchors = soup.select(\n",
    "                'ul[data-test=\"news-list\"] '\n",
    "                'li article a[data-test=\"article-title-link\"]'\n",
    "            )\n",
    "            return [a[\"href\"] for a in anchors if a.has_attr(\"href\")]\n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES:\n",
    "                backoff = 2 ** (attempt - 1) + random.random()\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                print(f\"Page {page} failed after {MAX_RETRIES}: {e}\")\n",
    "    return []\n",
    "\n",
    "def robust_scrape():\n",
    "    first = fetch_page(1)\n",
    "    PER_PAGE = len(first)\n",
    "    if PER_PAGE == 0:\n",
    "        raise RuntimeError(\"Failed to fetch the first page. Please check headers or cookies and try again.\")\n",
    "    print(f\"Detected {PER_PAGE} links per page, expecting {PER_PAGE * MAX_PAGE} total\")\n",
    "\n",
    "    results = {1: first}\n",
    "    pages = list(range(2, MAX_PAGE + 1))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "        futures = {pool.submit(fetch_page, p): p for p in pages}\n",
    "        for fut in as_completed(futures):\n",
    "            p = futures[fut]\n",
    "            results[p] = fut.result()\n",
    "\n",
    "        for round in range(1, MAX_RETRIES + 1):\n",
    "            bad = [p for p, links in results.items() if len(links) != PER_PAGE]\n",
    "            if not bad:\n",
    "                print(f\"All pages OK after {round-1} retries\")\n",
    "                break\n",
    "            print(f\"Retry round {round} for pages: {bad}\")\n",
    "            futures = {pool.submit(fetch_page, p): p for p in bad}\n",
    "            for fut in as_completed(futures):\n",
    "                p = futures[fut]\n",
    "                results[p] = fut.result()\n",
    "        else:\n",
    "            print(\"Retry limit reached; some pages may still be incomplete.\")\n",
    "\n",
    "    total_fetched = sum(len(links) for links in results.values())\n",
    "    expected = PER_PAGE * MAX_PAGE\n",
    "    print(f\"Total links fetched (including duplicates): {total_fetched} (expected {expected})\")\n",
    "\n",
    "    all_links = set(link for links in results.values() for link in links)\n",
    "    print(f\"Final: got {len(all_links)} unique URLs (expected {expected})\")\n",
    "    return list(all_links)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    links = robust_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf873f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fetch][5/20][ok]\n",
      "[Fetch][4/20][ok]\n",
      "[Fetch][17/20][ok]\n",
      "[Fetch][1/20][ok]\n",
      "[Fetch][16/20][ok]\n",
      "[Fetch][18/20][ok]\n",
      "[Fetch][12/20][ok]\n",
      "[Fetch][10/20][ok]\n",
      "[Fetch][2/20][ok]\n",
      "[Fetch][8/20][ok]\n",
      "[Fetch][9/20][ok]\n",
      "[Fetch][6/20][ok]\n",
      "[Fetch][15/20][ok]\n",
      "[Fetch][20/20][ok]\n",
      "[Fetch][13/20][ok]\n",
      "[Fetch][19/20][ok]\n",
      "[Fetch][11/20][ok]\n",
      "[Fetch][7/20][ok]\n",
      "[Fetch][14/20][ok]\n",
      "[Fetch][3/20][ok]\n",
      "[Process][1/20] https://www.investing.com/news/pro/td-cowen-maintains-tesla-at-buy-with-a-price-target-of-33000-4106202\n",
      "[Process][2/20] https://www.investing.com/news/stock-market-news/tesla-seeks-confidentiality-on-robotaxi-safety-answers-to-nhtsa-93CH-4106440\n",
      "[Process][3/20] https://www.investing.com/news/stock-market-news/nhtsa-contacts-tesla-on-robotaxi-issues-seen-in-online-videos-bloomberg-news-reports-4106795\n",
      "[Process][4/20] https://www.investing.com/news/stock-market-news/europeans-seek-digital-sovereignty-as-us-tech-firms-embrace-trump-4106318\n",
      "[Process][5/20] https://www.investing.com/news/analyst-ratings/tesla-price-target-raised-to-215-from-190-at-ubs-on-robotaxi-potential-93CH-4106287\n",
      "[Process][6/20] https://www.investing.com/news/stock-market-news/tesla-shares-soar-after-first-robotaxi-rides-hit-the-road-in-austin-texas-4106265\n",
      "[Process][7/20] https://www.investing.com/news/stock-market-news/tsx-futures-inch-higher-amid-fallout-from-us-strikes-on-iran-4105471\n",
      "[Process][8/20] https://www.investing.com/news/economy-news/trading-day-all-aboard-the-risk-on-rollercoaster-4106783\n",
      "[Process][9/20] https://www.investing.com/news/stock-market-news/tesla-wants-us-to-withhold-all-answers-on-robotaxi-deployment-from-public-view-4106419\n",
      "[Process][10/20] https://www.investing.com/news/stock-market-news/tesla-sued-over-new-jersey-crash-of-model-s-that-killed-three-4106828\n",
      "[Process][11/20] https://www.investing.com/news/pro/tesla-gains-6-amid-robotaxi-launch-432SI-4106111\n",
      "[Process][12/20] https://www.investing.com/news/pro/tesla-robotaxi-videos-show-speeding-driving-into-wrong-lane--bloomberg-432SI-4106437\n",
      "[Process][13/20] https://www.investing.com/news/stock-market-news/tesla-shares-rise-in-premarket-after-robotaxi-service-launch-4105077\n",
      "[Process][14/20] https://www.investing.com/news/stock-market-news/wall-street-futures-edge-higher-as-investors-await-irans-retaliatory-move-4105278\n",
      "[Process][15/20] https://www.investing.com/news/stock-market-news/tesla-stock-pares-gains-amid-reports-of-robotaxi-traffic-violations-4106447\n",
      "[Process][16/20] https://www.investing.com/news/analyst-ratings/tesla-stock-rises-as-td-cowen-reiterates-buy-rating-after-robotaxi-launch-93CH-4106304\n",
      "[Process][17/20] https://www.investing.com/news/pro/nhtsa-contacts-tesla-on-robotaxi-issues-seen-in-online-videos--bloomberg-432SI-4106760\n",
      "[Process][18/20] https://www.investing.com/news/pro/ubs-maintains-tesla-at-sell-with-a-price-target-of-21500-4106181\n",
      "[Process][19/20] https://www.investing.com/news/stock-market-news/wall-st-futures-rise-aftertrump-announces-israeliran-ceasefire-4106888\n",
      "[Process][20/20] https://www.investing.com/news/stock-market-news/us-stocks-higher-at-close-of-trade-dow-jones-industrial-average-up-089-4106722\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "FETCH_WORKERS = min(32, os.cpu_count() * 4)  \n",
    "PROCESS_WORKERS = os.cpu_count() or 4\n",
    "MAX_FETCH_RETRIES = 5                      \n",
    "RETRY_DELAY = 1                             \n",
    "TICKER = ticker\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "def is_placeholder(html: str) -> bool:\n",
    "    lower = html.lower() if html else \"\"\n",
    "    return (\n",
    "        'temporarily down for maintenance' in lower\n",
    "        or 'just a moment' in lower\n",
    "        or \"we're temporarily down\" in lower\n",
    "    )\n",
    "\n",
    "def safe_find_datetime(url, html_content=None):\n",
    "    try:\n",
    "        # Strategy 1: Use htmldate library to extract date from URL\n",
    "        dt = find_date(url)\n",
    "        if dt:\n",
    "            return dt, \"00:00\"  # Return with default time if date found\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if html_content:\n",
    "        # Strategy 2: Look for American format with AM/PM\n",
    "        m = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4}),\\s*(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            try:\n",
    "                dt = datetime.strptime(f\"{ds}, {ts}\", \"%m/%d/%Y, %I:%M %p\")\n",
    "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Strategy 3: Look for numeric dates with 24-hour time format\n",
    "        m = re.search(r\"(\\d{2}/\\d{2}/\\d{4}),\\s*(\\d{2}:\\d{2})\", html_content)\n",
    "        if m:\n",
    "            ds, ts = m.groups()\n",
    "            # Try both European and American date formats\n",
    "            for fmt in (\"%d/%m/%Y, %H:%M\", \"%m/%d/%Y, %H:%M\"):\n",
    "                try:\n",
    "                    dt = datetime.strptime(f\"{ds}, {ts}\", fmt)\n",
    "                    return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%H:%M\")\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M\")\n",
    "\n",
    "def fetch_html(url, idx, total):\n",
    "    for attempt in range(1, MAX_FETCH_RETRIES + 1):\n",
    "        try:\n",
    "            resp = scraper.get(url, timeout=30)\n",
    "            html = resp.text\n",
    "            if is_placeholder(html):\n",
    "                raise RuntimeError('Placeholder')\n",
    "                \n",
    "            print(f\"[Fetch][{idx}/{total}][ok]\")\n",
    "            return url, html\n",
    "            \n",
    "        except Exception:\n",
    "            print(f\"[Fetch][{idx}/{total}][retry {attempt}]\")\n",
    "            if attempt < MAX_FETCH_RETRIES:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                \n",
    "    print(f\"[Fetch error] {idx}/{total}: failed after {MAX_FETCH_RETRIES} retries\")\n",
    "    return url, None\n",
    "\n",
    "def process_article(arg):\n",
    "    url, html = arg\n",
    "    if not html:\n",
    "        return None\n",
    "        \n",
    "    art = Article(url)\n",
    "    art.set_html(html)\n",
    "    \n",
    "    try:\n",
    "        art.parse()\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    text = art.text or \"\"\n",
    "    title = (art.title or \"\").strip() or \"No title\"\n",
    "    \n",
    "    date, tm = safe_find_datetime(url, html)\n",
    "    \n",
    "    # Return combined data using dictionary unpacking\n",
    "    return {'ticker': TICKER, 'publish_date': date, 'publish_time': tm,\n",
    "             'title': title, 'body_text': text, 'url': url}\n",
    "\n",
    "async def scrape_all(urls):\n",
    "    total = len(urls)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Phase 1: Fetch HTML content from all URLs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as fetch_pool:\n",
    "        # Create fetch tasks and run them through the thread pool\n",
    "        fetch_tasks = [loop.run_in_executor(fetch_pool, fetch_html, u, i+1, total)\n",
    "                       for i, u in enumerate(urls)]\n",
    "        # Wait for all fetch tasks to complete\n",
    "        fetched = await asyncio.gather(*fetch_tasks)\n",
    "\n",
    "    # Phase 2: Process all fetched HTML content in parallel\n",
    "    records = []\n",
    "    with ThreadPoolExecutor(max_workers=PROCESS_WORKERS) as proc_pool:\n",
    "        # Submit processing tasks only for URLs with successful fetches\n",
    "        futures = {\n",
    "            proc_pool.submit(process_article, fr): fr[0]\n",
    "            for fr in fetched if fr[1]  # Skip URLs where HTML is None\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            res = fut.result()\n",
    "            print(f\"[Process][{i}/{total}] {futures[fut]}\")\n",
    "            if res:\n",
    "                records.append(res)\n",
    "                \n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# ——— Main entry point function ———\n",
    "def main(links):\n",
    "    df = asyncio.get_event_loop().run_until_complete(scrape_all(links))\n",
    "    return df\n",
    "\n",
    "# Execute the main function if this script is run directly\n",
    "if __name__ == '__main__':\n",
    "    df = main(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8556e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['publish_date', 'publish_time'], ascending=[False,False]).reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d94bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with empty body_text: 0\n"
     ]
    }
   ],
   "source": [
    "empty_body_count = df[df['body_text'] == ''].shape[0]\n",
    "print(f\"Number of articles with empty body_text: {empty_body_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52396323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY loaded successfully.\n",
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "load_dotenv(\"../SentimentAnalysis/GPT/secret.env\")\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "mongo_connection_string = os.getenv(\"MONGO_CONNECTION_STRING\")\n",
    "\n",
    "if api_key is None:\n",
    "    print(\"Error: GEMINI_API_KEY not found in .env file or environment variables.\")\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY loaded successfully.\")\n",
    "\n",
    "try:\n",
    "    client = MongoClient(mongo_connection_string)\n",
    "    db = client['stock_news_db']\n",
    "    collection = db['news_data']\n",
    "\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = genai.GenerationConfig(\n",
    "        temperature=0,\n",
    ")\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-preview-04-17\", generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd63ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "def is_retryable(e) -> bool:\n",
    "    if retry.if_transient_error(e):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ClientError) and e.code == 429):\n",
    "        return True\n",
    "    elif (isinstance(e, genai.errors.ServerError) and e.code == 503):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "@retry.Retry(predicate=is_retryable)\n",
    "def generate_content_with_rate_limit(prompt):\n",
    "  return model.generate_content(prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf008dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a financial news analyst specializing in stock market impact. Your task is to analyze the provided news article, summarize its core content concisely, determine its sentiment (positive, negative, or neutral), and assess its importance to the specified stock.\n",
    "\n",
    "Here is the news from stock [STOCK] title and body:\n",
    "---\n",
    "[TITLE]\n",
    "---\n",
    "[BODY]\n",
    "---\n",
    "\n",
    "Please provide your analysis in the following format (Don't forget to make space between the sections as shown):\n",
    "\n",
    "**Sentiment:**\n",
    "[Positive / Negative / Neutral]\n",
    "\n",
    "**Summary:**\n",
    "[Your concise summary of the article, typically 2-3 sentences.]\n",
    "\n",
    "**Reasoning for Sentiment:**\n",
    "[Brief explanation (1-2 sentences) of why you categorized the sentiment as such, referencing key points or tone from the article.]\n",
    "\n",
    "**Importance to Stock [STOCK]:**\n",
    "[1-5, where 1 is minimal importance and 5 is very high importance.Answer in 1-5 only, no explanation.] (Answer only in number 1-5)\n",
    "\n",
    "**Reasoning for Importance:**\n",
    "[Brief explanation (1-2 sentences) of why you assigned this importance score, referencing specific details from the article that would impact the stock.]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af69c3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [01:41<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "    current_stock = row.get(\"ticker\", \"news\")\n",
    "\n",
    "    filled_prompt = prompt.replace(\"[STOCK]\", current_stock)\n",
    "    filled_prompt = filled_prompt.replace(\"[TITLE]\", row[\"title\"])\n",
    "    filled_prompt = filled_prompt.replace(\"[BODY]\", row[\"body_text\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            predicted.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            predicted.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        predicted.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            predicted.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             predicted.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            predicted.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20b6b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted = np.array(predicted)\n",
    "df[\"predicted\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df145704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[1].strip() if len(x.split(\"\\n\")) > 1 else None)\n",
    "df[\"importance\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[10].strip() if len(x.split(\"\\n\")) > 10 else None)\n",
    "df[\"summary\"] = df[\"predicted\"].apply(lambda x: x.split(\"\\n\")[4].strip() if len(x.split(\"\\n\")) > 4 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8ae451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentiment'].isin(['Positive', 'Negative', 'Neutral'])]\n",
    "df = df[df['importance'].isin(['1', '2', '3', '4', '5'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c97e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash-001\", generation_config=generation_config)\n",
    "prompt = \"\"\"Translate the following English sentence to Thai. Do not translate proper nouns, company names, product names, abbreviations, or technical terms — keep them in English. Do not provide any explanation, just the translation.\n",
    "[TEXT]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027e30a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompting: 100%|██████████| 20/20 [00:24<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "translate = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Prompting\"): \n",
    "\n",
    "    filled_prompt = prompt.replace(\"[TEXT]\", row[\"summary\"])\n",
    "\n",
    "    try:\n",
    "        response = generate_content_with_rate_limit(filled_prompt)\n",
    "        finalprediction = response.strip()\n",
    "        if not finalprediction:\n",
    "            print(f\"Row {index}: LLM returned an empty string.\")\n",
    "            translate.append(\"LLM_EMPTY_RESPONSE\")\n",
    "        else:\n",
    "            translate.append(finalprediction)\n",
    "    except ValueError as ve:\n",
    "        print(f\"Row {index}: ValueError - {ve}. Appending 'ERROR_VALUE_ERROR'.\")\n",
    "        translate.append(\"ERROR_VALUE_ERROR\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        if \"429 Too Many Requests\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(f\"Row {index}: Rate Limit Exceeded or Quota Error - {e}. Appending 'ERROR_RATE_LIMIT'.\")\n",
    "            translate.append(\"ERROR_RATE_LIMIT\")\n",
    "        elif \"safety\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "             print(f\"Row {index}: Content Safety/Blocked - {e}. Appending 'ERROR_SAFETY_BLOCKED'.\")\n",
    "             translate.append(\"ERROR_SAFETY_BLOCKED\")\n",
    "        else:\n",
    "            print(f\"Row {index}: Unexpected Error - {e}. Appending 'ERROR_UNEXPECTED'.\")\n",
    "            translate.append(\"ERROR_UNEXPECTED\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b227831",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER = current_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07fcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predicted' in df.columns:\n",
    "    df.drop(columns=['predicted'], inplace=True)\n",
    "if 'body_text' in df.columns:\n",
    "    df.drop(columns=['body_text'], inplace=True)\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d %H-%M\").strip().replace(' ', '_')\n",
    "filename = f\"Gemini_{TICKER}_{date_time}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"translate\"] = translate\n",
    "df.to_csv(f\"Data/{filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f95d2b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted document with id: [ObjectId('685a277022b8db3ec7f044e4'), ObjectId('685a277022b8db3ec7f044e5'), ObjectId('685a277022b8db3ec7f044e6'), ObjectId('685a277022b8db3ec7f044e7'), ObjectId('685a277022b8db3ec7f044e8'), ObjectId('685a277022b8db3ec7f044e9'), ObjectId('685a277022b8db3ec7f044ea'), ObjectId('685a277022b8db3ec7f044eb'), ObjectId('685a277022b8db3ec7f044ec'), ObjectId('685a277022b8db3ec7f044ed'), ObjectId('685a277022b8db3ec7f044ee'), ObjectId('685a277022b8db3ec7f044ef'), ObjectId('685a277022b8db3ec7f044f0'), ObjectId('685a277022b8db3ec7f044f1'), ObjectId('685a277022b8db3ec7f044f2'), ObjectId('685a277022b8db3ec7f044f3'), ObjectId('685a277022b8db3ec7f044f4'), ObjectId('685a277022b8db3ec7f044f5'), ObjectId('685a277022b8db3ec7f044f6'), ObjectId('685a277022b8db3ec7f044f7')]\n"
     ]
    }
   ],
   "source": [
    "complete_dict=df.to_dict(orient='records')\n",
    "\n",
    "result = collection.insert_many(complete_dict,ordered=True)\n",
    "print(f\"Successfully inserted document with id: {result.inserted_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2445b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to be passed to Streamlit: ../CompletePipeline/Data/Gemini_TSLA_2025-06-24_11-20.csv\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.0.2:8501\u001b[0m\n",
      "\u001b[0m\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_filepath_for_streamlit = os.path.join(\"..\", \"CompletePipeline\", \"Data\", filename)\n",
    "\n",
    "print(f\"Path to be passed to Streamlit: {data_filepath_for_streamlit}\")\n",
    "\n",
    "!streamlit run ../Visualization/stock_app.py \"{data_filepath_for_streamlit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b69f69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>importance</th>\n",
       "      <th>summary</th>\n",
       "      <th>translate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-06-09</td>\n",
       "      <td>10:02</td>\n",
       "      <td>Edgewater comments on Meta Platforms By Invest...</td>\n",
       "      <td>https://www.investing.com/news/pro/edgewater-c...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>The provided text consists entirely of standar...</td>\n",
       "      <td>ข้อความที่ให้มาประกอบด้วยข้อความเปิดเผยความเสี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-06-09</td>\n",
       "      <td>06:06</td>\n",
       "      <td>Apple faces AI, regulatory challenges as it wo...</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>5</td>\n",
       "      <td>The article details the significant technical ...</td>\n",
       "      <td>บทความนี้ให้รายละเอียดเกี่ยวกับความท้าทายทางเท...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-06-09</td>\n",
       "      <td>05:24</td>\n",
       "      <td>Meta Platforms stock target reiterated at $750...</td>\n",
       "      <td>https://www.investing.com/news/analyst-ratings...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>4</td>\n",
       "      <td>Citizens JMP analysts reiterated their Market ...</td>\n",
       "      <td>นักวิเคราะห์ของ Citizens JMP ย้ำคำแนะนำ Market...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-06-09</td>\n",
       "      <td>04:39</td>\n",
       "      <td>Facebook Inc receives Investment Bank Analyst ...</td>\n",
       "      <td>https://www.investing.com/news/pro/citizens-jm...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>The article announces that Facebook Inc (META)...</td>\n",
       "      <td>บทความประกาศว่า Facebook Inc (META) ได้รับการป...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>META</td>\n",
       "      <td>2025-06-08</td>\n",
       "      <td>19:59</td>\n",
       "      <td>South Korean conservatives looking for rebirth...</td>\n",
       "      <td>https://www.investing.com/news/world-news/sout...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>South Korea's conservative People Power Party ...</td>\n",
       "      <td>พรรคพลังประชาชน (People Power Party) ฝ่ายอนุรั...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker publish_date publish_time  \\\n",
       "0   META   2025-06-09        10:02   \n",
       "1   META   2025-06-09        06:06   \n",
       "2   META   2025-06-09        05:24   \n",
       "3   META   2025-06-09        04:39   \n",
       "4   META   2025-06-08        19:59   \n",
       "\n",
       "                                               title  \\\n",
       "0  Edgewater comments on Meta Platforms By Invest...   \n",
       "1  Apple faces AI, regulatory challenges as it wo...   \n",
       "2  Meta Platforms stock target reiterated at $750...   \n",
       "3  Facebook Inc receives Investment Bank Analyst ...   \n",
       "4  South Korean conservatives looking for rebirth...   \n",
       "\n",
       "                                                 url sentiment importance  \\\n",
       "0  https://www.investing.com/news/pro/edgewater-c...   Neutral          1   \n",
       "1  https://www.investing.com/news/stock-market-ne...  Negative          5   \n",
       "2  https://www.investing.com/news/analyst-ratings...  Positive          4   \n",
       "3  https://www.investing.com/news/pro/citizens-jm...   Neutral          1   \n",
       "4  https://www.investing.com/news/world-news/sout...  Negative          1   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The provided text consists entirely of standar...   \n",
       "1  The article details the significant technical ...   \n",
       "2  Citizens JMP analysts reiterated their Market ...   \n",
       "3  The article announces that Facebook Inc (META)...   \n",
       "4  South Korea's conservative People Power Party ...   \n",
       "\n",
       "                                           translate  \n",
       "0  ข้อความที่ให้มาประกอบด้วยข้อความเปิดเผยความเสี...  \n",
       "1  บทความนี้ให้รายละเอียดเกี่ยวกับความท้าทายทางเท...  \n",
       "2  นักวิเคราะห์ของ Citizens JMP ย้ำคำแนะนำ Market...  \n",
       "3  บทความประกาศว่า Facebook Inc (META) ได้รับการป...  \n",
       "4  พรรคพลังประชาชน (People Power Party) ฝ่ายอนุรั...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
